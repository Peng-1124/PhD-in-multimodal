12.12 learning report

通过整体的视觉和语言介绍

文章链接：UNITER: UNiversal Image-TExt Representation Learning

ECCV 2020: SOTA on 9 VL tasks

文章的目的是，想学习所有vision+language的通用图像表示。

大多数视觉和语言任务依赖于联合多模型嵌入来弥合图像和文本线索之间的语义差距。文章提出了Universal Image-Text Representation(UNITER),一种用于联合多模态嵌入的大规模预训练模型。用transform作为模型的核心，利用注意力机制设计学习语境话表示。收到BERT启发，成功的将Transform用到NLP领域

![image](https://github.com/Peng-1124/PhD-in-multimodal/blob/master/figure/figure1%20of%201215.png)
这个网络第一步将图像和文本编码到一个具有图像嵌入器和文本嵌入器的公共嵌入空间，然后利用transform通过预训练模型学习每个文本和图像的嵌入。通过验证九个数据集发现表现优异。主要的成果：UNITER,是一个强大的V+L通用表示。

实现概述：给定图像和句子，统一将图像的视觉区域和句子的文本标记作为输入，设计了一个图像和文本的embedding来提取各自的嵌入。预训练模型主要有四个任务：Masked Language modeling conditioned on image regions(MLM),Masked Region Modeling conditioned on input text(with three variants)(MRM), Image-Text Matching(ITM), and Word-Region Alignment(WRA). MLM 和 MRM类似于两种掩码，我认为它与BERT类似。IMT负责学习整个图像和句子之间的对齐。预训练的四个模型都为了提高性能。



12.16 learning report

文章链接：Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Model

本篇文章主要讲预训练模型，UNITER是我之前看到过的一种，VLBERT.这两种属于单流模型，就是一个单个的transformer被应用在图像和文本两种模态。还有双流模型，就是两个transformer分别被应用在图像和文本上.LXMERT和ViLBERT.

BERT是一个预训练的模型，那么什么是预训练呢？举例子进行简单的介绍
假设已有A训练集，先用A对网络进行预训练，在A任务上学会网络参数，然后保存以备后用，当来一个新的任务B，采取相同的网络结构，网络参数初始化的时候可以加载A学习好的参数，其他的高层参数随机初始化，之后用B任务的训练数据来训练网络，当加载的参数保持不变时，称为"frozen"，当加载的参数随着B任务的训练进行不断的改变，称为“fine-tuning”，即更好地把参数进行调整使得更适合当前的B任务. 优点：当任务B的训练数据较少时，很难很好的训练网络，但是获得了A训练的参数，会比仅仅使用B训练的参数更优

本篇文章主要还是讲V+L的预训练模型，主要讲了五个问题。

1.What is the correlation between multimodal fusion and the number of layers in pre-trained models?（在预训练模型中，多模态融合与层数之间的相关性是什么？）更深的层次导致更多的多模态融合

2.Which modality plays a more important role that drives the pre-trained model to make fifinal predictions?（哪种模态在驱动预先训练的模型做出最终预测方面起着更重要的作用）文本模态比图像模态更重要

3.What knowledge is encoded in pre-trained models that supports cross-modal interaction and alignment?（支持跨模态交互和对齐的预训练模型中编码了哪些知识）

4.What knowledge has been learned for image-to-image (intra-modal) in-teraction (*i.e.*, visual relations) 在交互作用中，图像对图像（模态内）学习了哪些知识 跨膜态融合记录了视觉关系，为了评估从图像模态中学习到的编码知识，我们采用视觉关系检测任务，需要一个模型来识别和分类两个图像区域之间的关系。此任务可以被视为正在检查模型是否捕获图像区域之间的视觉关系。

5.Compared with BERT, do pre-trained V+L models effffectively encode linguistic knowledge for text-to-text (intra-modal) interaction（与BERT相比，预先训练好的V+L模型是否能有效地为文本间（模态内）交互编码语言知识？）预先训练好的模型有更好的语义
