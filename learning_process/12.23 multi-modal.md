12.23 多模态

#### field



#### AVSR

模态研究最早的例子之一是**视听语音识别**（audio-visual speech recognition avsr）。它的灵感来自麦格克效应（McGurk effect）[138]——在语音感知过程中听觉和视觉之间的相互作用。当受试者在观看一个人说/ga-ga/时听到音节/ba-ba/，他们感觉到第三个声音是/da-da/。在给自愿者放映的一部影片中，一个音节“ga”在配音时发作了“ba”，而自愿者称听到的音节是却是“da”。这样一来，视听信息联手创造出了第三种全新的声音，这个过程现在被叫做“麦格克效应”。 这是大脑对于来自眼睛和耳朵所提供的矛盾信息的努力猜测，这个理论也证明眼睛（**视觉信息**）对于大脑意识与知觉的影响比其他感觉器官所提供的信息更大。另一项研究发现，视觉信息的不一致可以改变对于口语发音的感知，这表明了麦格克效应可能在人们生活中许多外在感知上产生影响。

不过现在视听语音识别的方面比较卷。



#### 表征学习（Representation Learning）

表征学习是多模态任务的基础，其中包含了一些开放性问题，例如：如何结合来源不同的异质数据，如何处理不同模态的不同噪声等级，测试样本的某种模态缺失怎么办。现有多模态表征学习可分为两类：Joint(联合，也称为单塔结构)和Coordinated(协作，双塔结构)。12.16学习的多模态信息的表示方法就是其中之一，先设计一种结构把两种不同模态的信息用一种特征向量来表示。这是大部分多模态学习索要面临任务的基础，怎么把各个子空间的向量投影到公共子空间





#### 多模态融合（Multimodal Fusion）

多模态融合融合一般是指两个或者两个以上的模态的各种形式的组合，对于每一种信息的来源或者形式，都可以称为一种模态。目前研究领域中主要是对图像，文本，语言三种模态的处理。之所以要对模态进行融合，是因为不同模态的表现方式不一样，看待事物的角度也会不一样，所以存在一些交叉（所以存在信息冗余），互补（所以比单特征更优秀）的现象，甚至模态间可能还存在多种不同的信息交互，如果能合理的处理多模态信息，就能得到丰富特征信息。即概括来说多模态的显著特点是： 冗余性 和 互补性。

现在多模态融合主要有以下几种方法，基于向量的融合方法，基于神经网络的融合方法等等。基于神经网络的融合方法现在是主流，GAN更是大火，应对三个及三个以上模态的融合，GAN还没有很好的应对方法。

多模态融合现在应用的领域有很多，情感分析，多模态生物信息检测等等，多模态融合的方法是许多多模态应用中不可缺省的。下面会有详细介绍。



#### 多模态对齐（Multimodal Alignment）

多模态对齐的目的是挖掘多模态数据的子元素之间的关联性。对齐广泛应用于多模态任务中，具体的方式包括显式对齐和隐式对齐

1. **显示对齐。**如果一个模型的优化目标是最大化多模态数据的子元素的对齐程度，则称为显示对齐。包括无监督和有监督方法。**无监督对齐**：给定两个模态的数据作为输入，希望模型实现子元素的对齐，但是训练数据没有“对齐结果”的标注，模型需要同时学习相似度度量和对齐方式。而**有监督方法**存在标注，可训练模型学习相似度度量。Visual grounding便是有监督对齐的任务，而weakly-supervised visual grounding是无监督对齐的任务。
2. **隐式对齐。**如果模型的最终优化目标不是对齐任务，对齐过程仅仅是某个中间(或隐式)步骤，则称为隐式对齐。早期**基于概率图模型(如HMM)的方法**被应用于文本翻译和音素识别中，通过对齐源语言和目的语言的单词或声音信号与音素。但是他们都需要手动构建模态间的映射。最受欢迎的方式是**基于注意力机制的对齐，**我们对两种模态的子元素间求取注意力权重矩阵**，**可视为隐式地衡量跨模态子元素间的关联程度。在图像描述，这种注意力被用来判断生成某个单词时需要关注图像中的哪些区域。在视觉问答中，注意力权重被用来定位问题所指的图像区域。很多基于深度学习的跨模态任务都可以找到跨模态注意力的影子。

**讨论：**对齐可以作为一个单独的任务，也可以作为其他任务的隐式特征增强手段。多模态对齐可挖掘子元素间的细粒度交互，同时有可解释性，被广泛应用。但多模态对齐面临如下挑战：仅有少量数据集包含显式的对齐标注；跨模态度量难以设计；可能存在多种对齐，也可能存在某些元素无法在其他模态中找到。

链接地址：https://zhuanlan.zhihu.com/p/389287751



#### 多模态翻译（Multimodal Translation）

简单来说，多模态翻译指的是使用**语言以及其内容相关的图片**来进行翻译。在这里将其与普通的翻译任务进行区分：**传统的翻译任务**是给定模型一个源语言之后，输出其目标语言(target language)，比如说中文翻译成英文的任务中，源语言就是一段中文，目标语言就是一段英文。而在**多模态翻译任务**中，我们给模型不仅有源语言，还有输入的内容相关的图片，输出就是目标语言，比如同样是英文翻译成中文的任务中，多模态翻译任务的输入是：I love China.以及一张中国国旗的图片，则其输出就是：我爱中国。

多模机器学习的很大一部分涉及从一种形式到另一种形式的翻译（映射）。给定一个模态中的实体，任务是用不同的模态生成相同的实体。例如，给定一个图像，我们可能希望生成一个描述它的句子，或者给定一个文本描述，生成一个匹配它的图像。多模态翻译是一个长期研究的问题，在语音合成、视觉语音生成、视频描述、跨模态检索等领域都有早期的工作。

近年来，由于计算机视觉和自然语言处理(NLP)社区的共同努力，以及大型多模态数据集最近的可用性，多模态翻译重新引起了人们的兴趣。一个特别受欢迎的问题是视觉场景描述，也称为图像和视频字幕，它是许多计算机视觉和NLP问题的一个很好的测试平台。要解决这一问题，我们不仅要充分理解视觉场景，识别其突出的部分，而且要在语法上正确、全面而简洁的描述它的句子。虽然多模态翻译的方法非常广泛，而且通常是模态特有的，但它们有许多共同的因素。我们将它们分为两类——基于实例的和生成的。基于实例的模型在模式之间转换时使用字典。

。



#### Application



#### 跨膜态检索（Cross-Modal Retrieval）

跨模态检索是对一种模态的查询词，返回与之相关的其他不同模态检索结果的新型检索方法，是跨媒体检索的新兴技术。通过分析跨模态检索的实际需求，给出了跨模态检索问题的定义，综述了目前主流跨模态检索方法的核心思想，列举了常用数据集与评价方法，最后分析了跨模态检索存在的问题以及未来研究趋势。跨模态检索的主流方法大致可以分为四类：子空间的方法、深度学习的方法、哈希变换的方法和主题模型的方法

与传统的单模态检索不同，在跨模态检索中，检索结果的模态和查询的模态是不同的。比如，用户使用图像检索文本，视频和音频。跨模态检索的关键在于对不同模态的关系进行建模，难点就是跨越语义鸿沟。然而，当要检索的文档包含多模态的时候，一般的跨模态方法就无法直接应用到多模态检索。

多模态检索方法可以处理带有多个模态的多媒体数据，在多模态检索中，查询和要检索的文档可能包含不止一个模态。多模态检索方法可以用来提高单模态检索的准确度。多模态和跨模态检索的主要区别在于: 在多模态检索中，查询和要检索的文档必须至少有一个模态是相同的。多模态方法通常是融合不同的模态进行检索，而不是对他们的关系进行建模。比如，在许多多模态图像检索系统中，查询图像可能都有相关的文本，要检索的图像也包含相关的文本信息。而如果查询和要检索的文档没有相同的模态，那么这就是跨模态要解决的问题，传统的多模态方法就无能为力了。

#### 情感分析（emotion analysis）

随着社交网络的快速发展，人们在平台上的表达方式变得越来越丰富，如通过图文和视频表达自己的情绪和观点。如何分析多模态数据（本文指声音，图像和文字，不涉及传感器数据）中的情感，是当前情感分析领域面临的机遇和挑战。

一方面，以往情感分析聚焦于单个模态。如文本情感分析着眼于分析，挖掘和推理文本中蕴含的情感。现在需要对多个模态的数据进行处理和分析，这给研究人员带来了更大的挑战。另一方面，多模态数据与单模态数据相比，包含了更多的信息，多个模态之间可以互相补充。例如，在识别这条推文是否为反讽，“今天天气真好！”。如果只从文本来看，不是反讽。而如果其附加一张阴天的图片，可能就是反讽。不同模态信息相互补充，可以帮助机器更好地理解情感。从人机交互角度出发，多模态情感分析可以使得机器在更加自然的情况下与人进行交互。机器可以基于图像中人的表情和手势，声音中的音调，和识别出的自然语言来理解用户情感，进而进行反馈。

综上来讲，多模态情感分析技术的发展源于实际生活的需求，人们以更加自然的方式表达情感，技术就应有能力进行智能的理解和分析。虽然多模态数据包含了更多的信息，但如何进行多模态数据的融合，使得利用多模态数据能够提升效果，而不是起了反作用。如何利用不同模态数据之间的对齐信息，建模不同模态数据之间关联，如人们听见“喵”就会想起猫。这些都是当前多模态情感分析领域感兴趣的问题。。

通过不同模态组合（图文：文本+图片，视频：文本+图片+音频）来梳理相关的研究任务，对于文本+音频这种组合方式少有特意构建的相关数据集，一般通过对语音进行ASR或者使用文本+图片+音频中的文本+音频来构造数据集。对于文本+音频，语音方向的研究工作较多，所以本文暂未涉及。如表1所示，面向图文的情感分析任务有面向图文的情感分类任务，面向图文的方面级情感分类任务和面向图文的反讽识别任务。面向视频的情感分析任务有面向评论视频的情感分类任务，面向新闻视频的情感分类任务，面向对话视频的情感分类任务和面向对话视频的反讽识别任务。
![image}(https://github.com/Peng-1124/PhD-in-multimodal/blob/master/figure/figure1%20of%201223.png)





#### 文本生成视频(TEXT-TO-IMAGE)

用CNN,GAN做的比较多，2017年开始就比较火



#### 机器人

目标导向机器人早期的融合

基于视觉和触觉的机器人

机器人在复杂场景中的移动等等



#### 自动驾驶

过去几年，我们见证了自动驾驶的快速发展。然而，由于复杂和动态的驾驶环境，目前实现完全自动驾驶仍然是一项艰巨的任务。因此，自动驾驶汽车配备了一套传感器来进行强大而准确的环境感知。随着传感器的数量和类型不断增加，将它们融合来更好地感知环境正在成为一种趋势。所以多模态也被应用在自动驾驶的传感器上。

动驾驶汽车（AV）通常配备一个感知子系统来实时检测和跟踪运动目标。感知子系统是将来自一组传感器的数据作为输入，经过一系列的处理步骤后，输出关于环境、其他物体（如汽车）以及自动驾驶汽车本身的知识。AV上的传感器通常包括摄像头、激光雷达（Light Detection And Ranging sensor，LiDAR）、雷达（Radio detection and ranging，Radar）、GPS（Global Positioning System）、惯性测量单元(inertial measurement units)等。

具体来说，感知子系统有三个基本要求。

首先，它需要是准确的，并给出了驾驶环境的准确描述。

其次，具有鲁棒性。能在恶劣天气下、甚至当一些传感器退化甚至失效时保证AV的稳定与安全。

第三，实时性，能提供快速的反馈。

为了满足上述需求，感知子系统同时执行多个重要任务，如3D目标检测、跟踪、同步定位与映射(SLAM)等
